{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function, Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import ticker\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1120d2bb0>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting parameters\n",
    "plt.rc('font', family='serif', serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "plt.rc('axes', labelsize=10)\n",
    "plt.rc('legend', fontsize=10)\n",
    "\n",
    "width = 5\n",
    "height = width/1.618"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable Elastic-net regression\n",
    "\n",
    "The function `batch_solve` performs batched elastic-net regression. Given a K x N x n tensor of `Xs` data and K x N tensor `ys` of targets, the function converts the data into the matrices for the QP solved by elastic-net regression and returns the learned coefficient vectors and intercepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_solve(Xs, ys, lamb):\n",
    "    batch_size, N, n = Xs.size()\n",
    "\n",
    "    # since elastic-net does not penalize intercepts, find intercepts\n",
    "    intercepts = Variable(torch.zeros(batch_size, 1))\n",
    "    for i in range(batch_size):\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(Xs[i].data.numpy(), ys[i].data.numpy())\n",
    "        intercepts[i] = torch.FloatTensor([lr.intercept_[0]]).unsqueeze(-1)\n",
    "    \n",
    "    # form tensors X^T X and X^T (y-intercepts)\n",
    "    XtX = torch.bmm(Xs.transpose(1,2), Xs)\n",
    "    Xty = torch.bmm(Xs.transpose(1,2), (ys.squeeze(-1) - intercepts).unsqueeze(-1))\n",
    "    \n",
    "    # useful tensors\n",
    "    eye = Variable(torch.eye(n).repeat([batch_size, 1, 1]))\n",
    "    ones = Variable(torch.ones(n,1).repeat([batch_size, 1, 1]))\n",
    "    \n",
    "    # form QP parameters\n",
    "    Q = torch.cat([\n",
    "        torch.cat([1./N*XtX+lamb[1]*eye, -1./N*XtX],1),\n",
    "        torch.cat([-1./N*XtX, 1./N*XtX+lamb[1]*eye],1)\n",
    "    ],2)\n",
    "    p = torch.cat([-1./N*Xty+lamb[0]*ones, 1./N*Xty+lamb[0]*ones],1).squeeze()\n",
    "    G = Variable(-torch.eye(2*n).repeat([batch_size, 1, 1]))\n",
    "    h = Variable(torch.zeros(batch_size, 2*n))\n",
    "    e = Variable(torch.Tensor())\n",
    "    \n",
    "    # batch solve QP\n",
    "    z = QPFunction(verbose=-1)(Q,p,G,h,e,e)\n",
    "    \n",
    "    # return coefficient vector and intercepts\n",
    "    return z[:,:n] - z[:,n:], intercepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset\n",
    "\n",
    "Next, we construct artificial regression data with $N=1000$ and $n=10$, but only $8$ of the features are informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients [0.98982973 0.08412531 0.23580962 0.63365848 0.37441971 0.5469327\n",
      " 0.92164717 0.         0.29819623 0.        ]\n",
      "E[y] 0.005752656861007601\n",
      "std[y] 1.8610587417372477\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "n = 10\n",
    "X, y, coef = make_regression(N, n, n_informative=8, noise=100., \\\n",
    "                             tail_strength=0., coef=True)\n",
    "y /= 100.\n",
    "coef /= 100.\n",
    "print (\"coefficients\", coef)\n",
    "print (\"E[y]\", np.mean(y))\n",
    "print (\"std[y]\", np.std(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we select only 30 of the points to be training points and convert them to pytorch variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntrain: 30\n",
      "ntest: 970\n"
     ]
    }
   ],
   "source": [
    "N, n = X.shape\n",
    "\n",
    "ntrain = 30\n",
    "ntest = N-ntrain\n",
    "print (\"ntrain:\", ntrain)\n",
    "print (\"ntest:\", ntest)\n",
    "\n",
    "X_train, y_train = X[:ntrain,:], y[:ntrain]\n",
    "X_test, y_test = X[ntrain:,:], y[ntrain:]\n",
    "\n",
    "X_train = Variable(torch.FloatTensor(X_train), requires_grad=False)\n",
    "y_train = Variable(torch.FloatTensor(y_train), requires_grad=False).unsqueeze(-1)\n",
    "X_test = Variable(torch.FloatTensor(X_test), requires_grad=False)\n",
    "y_test = Variable(torch.FloatTensor(y_test), requires_grad=False).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the training data into $64$ random partitions with $27$ training points and $3$ validation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into K random partitions\n",
    "K = 128\n",
    "f = .95\n",
    "Xs_train = torch.zeros(K, int(ntrain*f), n)\n",
    "ys_train = torch.zeros(K, int(ntrain*f), 1)\n",
    "Xs_val = torch.zeros(K, ntrain-int(ntrain*f), n)\n",
    "ys_val = torch.zeros(K, ntrain-int(ntrain*f), 1)\n",
    "\n",
    "for i in range(K):\n",
    "    indices = np.random.permutation(ntrain)\n",
    "    training_idx, val_idx = indices[:int(ntrain*f)], indices[int(ntrain*f):]\n",
    "    training_idx = torch.LongTensor(training_idx)\n",
    "    val_idx = torch.LongTensor(val_idx)\n",
    "    # fit scaling to training data, but apply to all data.\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train[training_idx].data.numpy())\n",
    "    X_train1 = Variable(torch.Tensor(scaler.transform(X_train.data.numpy())))\n",
    "    Xs_train[i] = X_train1[training_idx]; ys_train[i] = y_train[training_idx]\n",
    "    Xs_val[i] = X_train1[val_idx]; ys_val[i] = y_train[val_idx]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train.data.numpy())\n",
    "X_train1 = Variable(torch.Tensor(scaler.transform(X_train.data.numpy())))\n",
    "X_test1 = Variable(torch.Tensor(scaler.transform(X_test.data.numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2847221015464214"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.data.numpy(), y_train.data.numpy())\n",
    "lr_loss = np.linalg.norm(lr.predict(X_test.data.numpy()) - y_test.data.numpy())**2 / y_test.shape[0]\n",
    "lr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss achieved by true coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.960613829513703"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = np.linalg.norm(np.dot(X_test.data.numpy(), coef) - y_test.data.numpy().flatten())**2 / y_test.size()[0]\n",
    "best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lamb = Variable(torch.FloatTensor([1e-2,1e-4]), requires_grad=True)\n",
    "optimizer = torch.optim.SGD([lamb], lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.611355304718018\n",
      "test loss: 1.222 | lamb_1: 0.01085 | lamb_2 0.00083\n",
      "5.558211326599121\n",
      "test loss: 1.216 | lamb_1: 0.01170 | lamb_2 0.00155\n",
      "5.390768051147461\n",
      "test loss: 1.209 | lamb_1: 0.01253 | lamb_2 0.00224\n",
      "5.199543476104736\n",
      "test loss: 1.203 | lamb_1: 0.01332 | lamb_2 0.00291\n",
      "5.038514137268066\n",
      "test loss: 1.198 | lamb_1: 0.01408 | lamb_2 0.00357\n",
      "4.863897800445557\n",
      "test loss: 1.192 | lamb_1: 0.01481 | lamb_2 0.00422\n",
      "4.927940368652344\n",
      "test loss: 1.187 | lamb_1: 0.01557 | lamb_2 0.00485\n",
      "4.998310089111328\n",
      "test loss: 1.182 | lamb_1: 0.01634 | lamb_2 0.00548\n",
      "4.671179294586182\n",
      "test loss: 1.177 | lamb_1: 0.01703 | lamb_2 0.00611\n",
      "4.652484893798828\n",
      "test loss: 1.173 | lamb_1: 0.01773 | lamb_2 0.00673\n",
      "4.54174280166626\n",
      "test loss: 1.168 | lamb_1: 0.01840 | lamb_2 0.00734\n",
      "4.808726787567139\n",
      "test loss: 1.164 | lamb_1: 0.01914 | lamb_2 0.00795\n",
      "4.699917793273926\n",
      "test loss: 1.160 | lamb_1: 0.01986 | lamb_2 0.00856\n",
      "4.765109062194824\n",
      "test loss: 1.156 | lamb_1: 0.02060 | lamb_2 0.00916\n",
      "4.6783294677734375\n",
      "test loss: 1.152 | lamb_1: 0.02133 | lamb_2 0.00975\n",
      "4.576916217803955\n",
      "test loss: 1.148 | lamb_1: 0.02204 | lamb_2 0.01033\n",
      "4.479945182800293\n",
      "test loss: 1.144 | lamb_1: 0.02273 | lamb_2 0.01089\n",
      "4.349320888519287\n",
      "test loss: 1.140 | lamb_1: 0.02340 | lamb_2 0.01145\n",
      "4.375519752502441\n",
      "test loss: 1.137 | lamb_1: 0.02409 | lamb_2 0.01199\n",
      "4.285048007965088\n",
      "test loss: 1.134 | lamb_1: 0.02476 | lamb_2 0.01252\n",
      "4.262395858764648\n",
      "test loss: 1.131 | lamb_1: 0.02542 | lamb_2 0.01306\n",
      "3.969301462173462\n",
      "test loss: 1.128 | lamb_1: 0.02602 | lamb_2 0.01359\n",
      "3.9536752700805664\n",
      "test loss: 1.125 | lamb_1: 0.02661 | lamb_2 0.01411\n",
      "4.32999849319458\n",
      "test loss: 1.122 | lamb_1: 0.02729 | lamb_2 0.01464\n",
      "4.130246162414551\n",
      "test loss: 1.120 | lamb_1: 0.02793 | lamb_2 0.01517\n",
      "4.050950050354004\n",
      "test loss: 1.117 | lamb_1: 0.02855 | lamb_2 0.01569\n",
      "4.109579086303711\n",
      "test loss: 1.115 | lamb_1: 0.02919 | lamb_2 0.01620\n",
      "4.125579833984375\n",
      "test loss: 1.112 | lamb_1: 0.02982 | lamb_2 0.01673\n",
      "4.162653923034668\n",
      "test loss: 1.110 | lamb_1: 0.03046 | lamb_2 0.01726\n",
      "4.101891994476318\n",
      "test loss: 1.107 | lamb_1: 0.03110 | lamb_2 0.01779\n",
      "4.037259101867676\n",
      "test loss: 1.105 | lamb_1: 0.03172 | lamb_2 0.01830\n",
      "3.970074415206909\n",
      "test loss: 1.103 | lamb_1: 0.03233 | lamb_2 0.01881\n",
      "3.907824754714966\n",
      "test loss: 1.101 | lamb_1: 0.03293 | lamb_2 0.01931\n",
      "3.8480544090270996\n",
      "test loss: 1.099 | lamb_1: 0.03352 | lamb_2 0.01980\n",
      "3.7884857654571533\n",
      "test loss: 1.097 | lamb_1: 0.03411 | lamb_2 0.02028\n",
      "3.7657597064971924\n",
      "test loss: 1.096 | lamb_1: 0.03469 | lamb_2 0.02076\n",
      "3.4572904109954834\n",
      "test loss: 1.094 | lamb_1: 0.03519 | lamb_2 0.02123\n",
      "3.4457380771636963\n",
      "test loss: 1.093 | lamb_1: 0.03570 | lamb_2 0.02169\n",
      "3.346522569656372\n",
      "test loss: 1.091 | lamb_1: 0.03620 | lamb_2 0.02215\n",
      "3.304234266281128\n",
      "test loss: 1.090 | lamb_1: 0.03668 | lamb_2 0.02260\n",
      "3.2553865909576416\n",
      "test loss: 1.089 | lamb_1: 0.03716 | lamb_2 0.02304\n",
      "3.1343834400177\n",
      "test loss: 1.087 | lamb_1: 0.03761 | lamb_2 0.02348\n",
      "3.096466064453125\n",
      "test loss: 1.086 | lamb_1: 0.03806 | lamb_2 0.02391\n",
      "3.080622673034668\n",
      "test loss: 1.085 | lamb_1: 0.03850 | lamb_2 0.02433\n",
      "3.0438120365142822\n",
      "test loss: 1.084 | lamb_1: 0.03894 | lamb_2 0.02475\n",
      "2.898885488510132\n",
      "test loss: 1.083 | lamb_1: 0.03935 | lamb_2 0.02516\n",
      "2.889848232269287\n",
      "test loss: 1.082 | lamb_1: 0.03976 | lamb_2 0.02557\n",
      "2.851823568344116\n",
      "test loss: 1.082 | lamb_1: 0.04017 | lamb_2 0.02597\n",
      "2.819805145263672\n",
      "test loss: 1.081 | lamb_1: 0.04057 | lamb_2 0.02637\n",
      "2.851841449737549\n",
      "test loss: 1.080 | lamb_1: 0.04098 | lamb_2 0.02677\n",
      "2.829739570617676\n",
      "test loss: 1.079 | lamb_1: 0.04138 | lamb_2 0.02716\n",
      "2.7985239028930664\n",
      "test loss: 1.079 | lamb_1: 0.04178 | lamb_2 0.02755\n",
      "2.7776968479156494\n",
      "test loss: 1.078 | lamb_1: 0.04218 | lamb_2 0.02794\n",
      "2.7915220260620117\n",
      "test loss: 1.077 | lamb_1: 0.04259 | lamb_2 0.02832\n",
      "2.8351314067840576\n",
      "test loss: 1.077 | lamb_1: 0.04300 | lamb_2 0.02871\n",
      "2.870361804962158\n",
      "test loss: 1.076 | lamb_1: 0.04343 | lamb_2 0.02909\n",
      "2.8615481853485107\n",
      "test loss: 1.076 | lamb_1: 0.04385 | lamb_2 0.02948\n",
      "2.942357301712036\n",
      "test loss: 1.075 | lamb_1: 0.04430 | lamb_2 0.02986\n",
      "2.9214510917663574\n",
      "test loss: 1.074 | lamb_1: 0.04474 | lamb_2 0.03024\n",
      "2.9742870330810547\n",
      "test loss: 1.074 | lamb_1: 0.04520 | lamb_2 0.03062\n",
      "2.946556806564331\n",
      "test loss: 1.073 | lamb_1: 0.04565 | lamb_2 0.03100\n",
      "2.8501713275909424\n",
      "test loss: 1.073 | lamb_1: 0.04608 | lamb_2 0.03137\n",
      "2.8552300930023193\n",
      "test loss: 1.072 | lamb_1: 0.04652 | lamb_2 0.03174\n",
      "2.832441806793213\n",
      "test loss: 1.072 | lamb_1: 0.04695 | lamb_2 0.03211\n",
      "2.8524742126464844\n",
      "test loss: 1.072 | lamb_1: 0.04738 | lamb_2 0.03248\n",
      "2.821016311645508\n",
      "test loss: 1.071 | lamb_1: 0.04781 | lamb_2 0.03285\n",
      "2.789885997772217\n",
      "test loss: 1.071 | lamb_1: 0.04824 | lamb_2 0.03321\n",
      "2.7634849548339844\n",
      "test loss: 1.071 | lamb_1: 0.04866 | lamb_2 0.03356\n",
      "2.752246856689453\n",
      "test loss: 1.070 | lamb_1: 0.04908 | lamb_2 0.03392\n",
      "2.7659661769866943\n",
      "test loss: 1.070 | lamb_1: 0.04950 | lamb_2 0.03427\n",
      "2.723071575164795\n",
      "test loss: 1.070 | lamb_1: 0.04992 | lamb_2 0.03462\n",
      "2.6964452266693115\n",
      "test loss: 1.070 | lamb_1: 0.05034 | lamb_2 0.03496\n",
      "2.668980121612549\n",
      "test loss: 1.070 | lamb_1: 0.05075 | lamb_2 0.03530\n",
      "2.641355514526367\n",
      "test loss: 1.069 | lamb_1: 0.05116 | lamb_2 0.03564\n",
      "2.6652045249938965\n",
      "test loss: 1.069 | lamb_1: 0.05157 | lamb_2 0.03598\n",
      "2.674550771713257\n",
      "test loss: 1.069 | lamb_1: 0.05199 | lamb_2 0.03631\n",
      "2.682433605194092\n",
      "test loss: 1.069 | lamb_1: 0.05241 | lamb_2 0.03665\n",
      "2.6584630012512207\n",
      "test loss: 1.069 | lamb_1: 0.05282 | lamb_2 0.03698\n",
      "2.643310546875\n",
      "test loss: 1.069 | lamb_1: 0.05324 | lamb_2 0.03731\n",
      "2.623047113418579\n",
      "test loss: 1.069 | lamb_1: 0.05365 | lamb_2 0.03763\n",
      "2.6066131591796875\n",
      "test loss: 1.069 | lamb_1: 0.05406 | lamb_2 0.03795\n",
      "2.6008224487304688\n",
      "test loss: 1.069 | lamb_1: 0.05447 | lamb_2 0.03828\n",
      "2.5957510471343994\n",
      "test loss: 1.070 | lamb_1: 0.05488 | lamb_2 0.03860\n",
      "2.6243364810943604\n",
      "test loss: 1.070 | lamb_1: 0.05530 | lamb_2 0.03891\n",
      "2.601677179336548\n",
      "test loss: 1.071 | lamb_1: 0.05571 | lamb_2 0.03923\n",
      "2.5973658561706543\n",
      "test loss: 1.071 | lamb_1: 0.05613 | lamb_2 0.03954\n",
      "2.7041828632354736\n",
      "test loss: 1.072 | lamb_1: 0.05657 | lamb_2 0.03984\n",
      "2.677898645401001\n",
      "test loss: 1.072 | lamb_1: 0.05701 | lamb_2 0.04015\n",
      "2.574845790863037\n",
      "test loss: 1.073 | lamb_1: 0.05743 | lamb_2 0.04045\n",
      "2.5518147945404053\n",
      "test loss: 1.073 | lamb_1: 0.05785 | lamb_2 0.04074\n",
      "2.524495840072632\n",
      "test loss: 1.074 | lamb_1: 0.05826 | lamb_2 0.04103\n",
      "2.4810383319854736\n",
      "test loss: 1.074 | lamb_1: 0.05867 | lamb_2 0.04132\n",
      "2.374056816101074\n",
      "test loss: 1.075 | lamb_1: 0.05905 | lamb_2 0.04160\n",
      "2.3478705883026123\n",
      "test loss: 1.075 | lamb_1: 0.05942 | lamb_2 0.04188\n",
      "2.33052921295166\n",
      "test loss: 1.076 | lamb_1: 0.05980 | lamb_2 0.04216\n",
      "2.3107306957244873\n",
      "test loss: 1.076 | lamb_1: 0.06017 | lamb_2 0.04244\n",
      "2.316617012023926\n",
      "test loss: 1.077 | lamb_1: 0.06054 | lamb_2 0.04271\n",
      "2.2973365783691406\n",
      "test loss: 1.077 | lamb_1: 0.06091 | lamb_2 0.04298\n",
      "2.2757716178894043\n",
      "test loss: 1.078 | lamb_1: 0.06128 | lamb_2 0.04325\n"
     ]
    }
   ],
   "source": [
    "val_losses, test_losses = [], []\n",
    "for k in range(1,100):\n",
    "    zs, intercepts = batch_solve(X_train1.repeat([1,1,1]), y_train.repeat([1,1,1]), lamb.detach())\n",
    "    Xz_minus_y = torch.mm(X_test1, zs.squeeze(0).unsqueeze(-1)) + intercepts - y_test\n",
    "    test_loss = torch.norm(Xz_minus_y)**2 / (X_test.size()[0])\n",
    "    z_test = zs.squeeze()\n",
    "    \n",
    "    def step():\n",
    "        optimizer.zero_grad()\n",
    "        zs, intercepts = batch_solve(Xs_train, ys_train, lamb)\n",
    "        Xz_minus_y = torch.bmm(Xs_val, zs.unsqueeze(-1)).squeeze() + intercepts - ys_val.squeeze()\n",
    "        loss = torch.norm(Xz_minus_y)**2 / (K * Xs_val.size()[1])\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(step)\n",
    "    if lamb[1].data.item() < 1e-7:\n",
    "        lamb.data[1] = torch.FloatTensor([1e-7])\n",
    "    if lamb[0].data.item() <= 0:\n",
    "        lamb.data[0] = torch.FloatTensor([1e-8])\n",
    "\n",
    "    print (torch.norm(lamb.grad.data).item())\n",
    "    print (\"test loss: %.3f | lamb_1: %.5f | lamb_2 %.5f\" \\\n",
    "           % (test_loss.item(), lamb[0].data.item(), lamb[1].data.item()))\n",
    "    test_losses.append(test_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2836827039718628\n",
      "1.283287525177002\n",
      "1.28243887424469\n",
      "1.2806226015090942\n",
      "1.2767685651779175\n",
      "1.2687300443649292\n",
      "1.2525817155838013\n",
      "1.2226037979125977\n",
      "1.1763238906860352\n",
      "1.1325914859771729\n",
      "1.1323691606521606\n",
      "1.1318926811218262\n",
      "1.1308776140213013\n",
      "1.1287450790405273\n",
      "1.1243939399719238\n",
      "1.1161717176437378\n",
      "1.1037893295288086\n",
      "1.1059244871139526\n",
      "1.13079833984375\n",
      "1.1540684700012207\n",
      "1.2074096202850342\n"
     ]
    }
   ],
   "source": [
    "lambda_1s = np.logspace(-4,-1,10)\n",
    "lambda_2s = np.logspace(-4,-1,10)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_test_loss = float(\"inf\")\n",
    "test_losses_es = []\n",
    "\n",
    "for lambda_1 in lambda_1s:\n",
    "    for lambda_2 in lambda_2s:\n",
    "        lamb = Variable(torch.FloatTensor([lambda_1,lambda_2]), requires_grad=False)\n",
    "        zs, intercepts = batch_solve(X_train1.repeat([1,1,1]), y_train.repeat([1,1,1]), lamb)\n",
    "        Xz_minus_y = torch.mm(X_test1, zs.squeeze(0).unsqueeze(-1)) + intercepts - y_test\n",
    "        test_loss = torch.norm(Xz_minus_y)**2 / (X_test.size()[0])\n",
    "        z_test = zs.squeeze()\n",
    "\n",
    "        zs, intercepts = batch_solve(Xs_train, ys_train, lamb)\n",
    "        Xz_minus_y = torch.bmm(Xs_val, zs.unsqueeze(-1)).squeeze() + intercepts - ys_val.squeeze()\n",
    "        loss = torch.norm(Xz_minus_y)**2 / (K * Xs_val.size()[1])\n",
    "\n",
    "        if loss.item() < best_val_loss:\n",
    "            best_val_loss = loss.item()\n",
    "            best_test_loss = test_loss.item()\n",
    "            print (best_test_loss)\n",
    "        test_losses_es.append(best_test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.282341718673706\n",
      "1.2604131698608398\n",
      "1.13634192943573\n",
      "1.1333262920379639\n",
      "1.0839056968688965\n",
      "1.083789587020874\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_test_loss_rs = float(\"inf\")\n",
    "test_losses_rs = []\n",
    "for _ in range(100):\n",
    "    lamb = Variable(torch.FloatTensor([10**np.random.uniform(-4,-1),10**np.random.uniform(-4,-1)]), requires_grad=False)\n",
    "    zs, intercepts = batch_solve(X_train1.repeat([1,1,1]), y_train.repeat([1,1,1]), lamb)\n",
    "    Xz_minus_y = torch.mm(X_test1, zs.squeeze(0).unsqueeze(-1)) + intercepts - y_test\n",
    "    test_loss = torch.norm(Xz_minus_y)**2 / (X_test.size()[0])\n",
    "    z_test = zs.squeeze()\n",
    "\n",
    "    zs, intercepts = batch_solve(Xs_train, ys_train, lamb)\n",
    "    Xz_minus_y = torch.bmm(Xs_val, zs.unsqueeze(-1)).squeeze() + intercepts - ys_val.squeeze()\n",
    "    loss = torch.norm(Xz_minus_y)**2 / (K * Xs_val.size()[1])\n",
    "\n",
    "    if loss.item() < best_val_loss:\n",
    "        best_val_loss = loss.item()\n",
    "        best_test_loss = test_loss.item()\n",
    "        print (best_test_loss)\n",
    "    test_losses_rs.append(best_test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAADWCAYAAAC+EWvzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncQohRJwkcQohRJwkcQohRJz+PwA+EwganHiCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a23ce5198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(test_losses_rs[:100], label='random search')\n",
    "plt.plot(test_losses, label='cvgm')\n",
    "plt.plot(test_losses_es, label='exhaustive search')\n",
    "plt.legend()\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('test loss')\n",
    "fig.set_size_inches(width, height)\n",
    "plt.savefig('figures/comparison.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvgm test: 1.080196499824524\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-e26c5459e0a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"cvgm test:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"rs test:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses_rs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"es test:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses_es\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print (\"cvgm test:\", test_losses[-1])\n",
    "print (\"rs test:\", test_losses_rs[100])\n",
    "print (\"es test:\", test_losses_es[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
